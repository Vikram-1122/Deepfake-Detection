{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcbc09f6-9948-4f1e-8fb5-4b9e8e682b18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Training]: 100%|█████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.58s/it]\n",
      "Epoch 1/10 [Validation]: 100%|███████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Train Loss: 0.7977, Train Acc: 0.6000, Val Loss: 0.4957, Val Acc: 0.8889\n",
      "New Best Model Saved at Epoch 1 with Val Loss: 0.4957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Training]: 100%|█████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.72s/it]\n",
      "Epoch 2/10 [Validation]: 100%|███████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: Train Loss: 0.7285, Train Acc: 0.6000, Val Loss: 0.4892, Val Acc: 1.0000\n",
      "New Best Model Saved at Epoch 2 with Val Loss: 0.4892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Training]: 100%|█████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.96s/it]\n",
      "Epoch 3/10 [Validation]: 100%|███████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: Train Loss: 0.6750, Train Acc: 0.7000, Val Loss: 0.5270, Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Training]: 100%|█████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.73s/it]\n",
      "Epoch 4/10 [Validation]: 100%|███████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: Train Loss: 0.6953, Train Acc: 0.5000, Val Loss: 0.5476, Val Acc: 0.8889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Training]: 100%|█████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.76s/it]\n",
      "Epoch 5/10 [Validation]: 100%|███████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: Train Loss: 0.5658, Train Acc: 0.9000, Val Loss: 0.5804, Val Acc: 0.7778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Training]: 100%|█████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.29s/it]\n",
      "Epoch 6/10 [Validation]: 100%|███████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: Train Loss: 0.5626, Train Acc: 0.8000, Val Loss: 0.5975, Val Acc: 0.6667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Training]: 100%|█████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.40s/it]\n",
      "Epoch 7/10 [Validation]: 100%|███████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: Train Loss: 0.4543, Train Acc: 0.8000, Val Loss: 0.7669, Val Acc: 0.2222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Training]: 100%|█████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.88s/it]\n",
      "Epoch 8/10 [Validation]: 100%|███████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: Train Loss: 0.4522, Train Acc: 0.8000, Val Loss: 0.7577, Val Acc: 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Training]: 100%|█████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.66s/it]\n",
      "Epoch 9/10 [Validation]: 100%|███████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: Train Loss: 0.3772, Train Acc: 1.0000, Val Loss: 0.7434, Val Acc: 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Training]: 100%|████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.28s/it]\n",
      "Epoch 10/10 [Validation]: 100%|██████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: Train Loss: 0.3630, Train Acc: 1.0000, Val Loss: 0.8273, Val Acc: 0.4444\n",
      "Metrics saved to 'metrics.pkl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.7977234125137329,\n",
       "  0.728481650352478,\n",
       "  0.6749986410140991,\n",
       "  0.6953437328338623,\n",
       "  0.5657631754875183,\n",
       "  0.5626345276832581,\n",
       "  0.4542670249938965,\n",
       "  0.4522484838962555,\n",
       "  0.3771844208240509,\n",
       "  0.3629794716835022],\n",
       " [0.6, 0.6, 0.7, 0.5, 0.9, 0.8, 0.8, 0.8, 1.0, 1.0],\n",
       " [0.49570000171661377,\n",
       "  0.4892173409461975,\n",
       "  0.5270419120788574,\n",
       "  0.5475667119026184,\n",
       "  0.5804303288459778,\n",
       "  0.5975461006164551,\n",
       "  0.7669001221656799,\n",
       "  0.7577447295188904,\n",
       "  0.7434298992156982,\n",
       "  0.8272726535797119],\n",
       " [0.8888888888888888,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.8888888888888888,\n",
       "  0.7777777777777778,\n",
       "  0.6666666666666666,\n",
       "  0.2222222222222222,\n",
       "  0.3333333333333333,\n",
       "  0.3333333333333333,\n",
       "  0.4444444444444444])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import pickle  # For saving metrics\n",
    "\n",
    "# Use GPU if available, otherwise fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define a Simplified Model (ResNet18)\n",
    "class SimpleResNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(SimpleResNet, self).__init__()\n",
    "        self.backbone = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# Initialize Model\n",
    "model = SimpleResNet(num_classes=2).to(device)\n",
    "\n",
    "# Data Transformations with Augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Load Datasets\n",
    "train_dataset = datasets.ImageFolder(\"C:/Users/Vikram/DFDC/data/final/train\", transform=transform)\n",
    "val_dataset = datasets.ImageFolder(\"C:/Users/Vikram/DFDC/data/final/val\", transform=transform)\n",
    "\n",
    "# Use DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "\n",
    "# Handle Class Imbalance with Weighted Loss\n",
    "num_classes = 2  # Number of classes (real and fake)\n",
    "train_counts = Counter([label for _, label in train_dataset])\n",
    "\n",
    "# Ensure all classes are represented\n",
    "class_sample_counts = [train_counts.get(cls, 0) for cls in range(num_classes)]\n",
    "\n",
    "# Avoid division by zero\n",
    "total_samples = sum(class_sample_counts)\n",
    "class_weights = [total_samples / (count if count > 0 else 1) for count in class_sample_counts]\n",
    "\n",
    "# Convert to tensor and move to the device\n",
    "class_weights = torch.tensor(class_weights).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training Loop with Metrics Logging\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    # Initialize lists to store metrics\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies = []  # Correctly initialized\n",
    "    val_accuracies = []    # Correctly initialized\n",
    "\n",
    "    best_val_loss = float(\"inf\")  # Initialize the best validation loss\n",
    "    best_epoch = 0  # Initialize the best epoch\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_loss, train_acc = 0, 0\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_acc += torch.sum(preds == labels).item()\n",
    "\n",
    "        # Calculate average training loss and accuracy\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_train_acc = train_acc / len(train_loader.dataset)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(avg_train_acc)\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_loss, val_acc = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Validation]\"):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_acc += torch.sum(preds == labels).item()\n",
    "\n",
    "        # Calculate average validation loss and accuracy\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_val_acc = val_acc / len(val_loader.dataset)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(avg_val_acc)\n",
    "\n",
    "        # Print Metrics\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.4f}, \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {avg_val_acc:.4f}\")\n",
    "\n",
    "        # Save the best model checkpoint\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_epoch = epoch + 1\n",
    "            torch.save(model.state_dict(), \"m2tr_best_model.pth\")\n",
    "            print(f\"New Best Model Saved at Epoch {best_epoch} with Val Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    # Save metrics to a file for plotting later\n",
    "    with open(\"metrics.pkl\", \"wb\") as f:\n",
    "        pickle.dump((train_losses, train_accuracies, val_losses, val_accuracies), f)\n",
    "        print(\"Metrics saved to 'metrics.pkl'\")\n",
    "\n",
    "    return train_losses, train_accuracies, val_losses, val_accuracies\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17975d4c-00df-4167-94fa-634bcddc4b58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
